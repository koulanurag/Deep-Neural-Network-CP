from __future__ import print_function
import os
from conformal import ConformalPrediction
from conformal.measures import Ratio, Binary, Diff
from keras.datasets import cifar10
from keras.models import Sequential
from keras.preprocessing.image import ImageDataGenerator
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D
from keras.optimizers import RMSprop
from keras.utils import np_utils
from keras.models import load_model
from keras.models import Model
import keras.backend as K

try:
    import _pickle as cPickle
except:
    import cPickle
import matplotlib as mpl

mpl.use('Agg')  # to plot graphs over a server shell since the default display is not available on server.
import matplotlib.pyplot as plt

log = True
true_weight, false_weight = 0.9, 0.1


def _create_model(img_channels, img_rows, img_cols, nb_classes):
    model = Sequential()

    model.add(Convolution2D(32, 3, 3, input_shape=(img_channels, img_rows, img_cols), border_mode='same'))
    model.add(Activation('relu'))
    model.add(Convolution2D(32, 3, 3, border_mode='same'))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(BatchNormalization(epsilon=0.001, mode=0, axis=-1, momentum=0.99, weights=None, beta_init='zero',
                                 gamma_init='one', gamma_regularizer=None, beta_regularizer=None))
    model.add(Dropout(0.25))

    model.add(Convolution2D(64, 3, 3, border_mode='same'))
    model.add(Activation('relu'))
    model.add(Convolution2D(64, 3, 3, border_mode='same'))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(BatchNormalization(epsilon=0.001, mode=0, axis=-1, momentum=0.99,
                                 weights=None, beta_init='zero', gamma_init='one',
                                 gamma_regularizer=None, beta_regularizer=None))
    model.add(Dropout(0.25))

    model.add(Convolution2D(128, 3, 3, border_mode='same'))
    model.add(Activation('relu'))
    model.add(Convolution2D(128, 3, 3, border_mode='same'))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(BatchNormalization(epsilon=0.001, mode=0, axis=-1, momentum=0.99,
                                 weights=None, beta_init='zero', gamma_init='one',
                                 gamma_regularizer=None, beta_regularizer=None))
    model.add(Dropout(0.25))

    model.add(Flatten())
    model.add(Dense(512))
    model.add(Activation('relu'))
    model.add(Dense(nb_classes))
    model.add(Activation('sigmoid'))
    return model


def _data_augmentation_train(model, X_train, Y_train, X_test, Y_test, batch_size, nb_epoch):
    # this will do preprocessing and realtime data augmentation
    datagen = ImageDataGenerator(
        featurewise_center=True,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=True,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=False,  # randomly flip images
        vertical_flip=False)  # randomly flip images

    # compute quantities required for featurewise normalization
    # (std, mean, and principal components if ZCA whitening is applied)
    datagen.fit(X_train)

    # fit the model on the batches generated by datagen.flow()
    history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),
                                  samples_per_epoch=X_train.shape[0],
                                  nb_epoch=nb_epoch,
                                  validation_data=(X_test, Y_test),
                                  nb_worker=1)
    return history


def train(model, X_train, Y_train, X_test, Y_test, batch_size, nb_epoch, model_save_path, history_save_path,
          data_augmentation=True):
    if not data_augmentation:
        history = model.fit(X_train, Y_train, batch_size=batch_size,
                            nb_epoch=nb_epoch,
                            validation_data=(X_test, Y_test), shuffle=True)
    else:
        history = _data_augmentation_train(model, X_train, Y_train, X_test, Y_test, batch_size, nb_epoch)

    cPickle.dump({key: history.history[key][-1] for key in history.history.keys()}, open(history_save_path, 'wb'))
    model.save(model_save_path)
    return model, history


def _ensure_directory_exits(directory_path):
    """creates directory if path doesn't exist"""
    if not os.path.exists(directory_path):
        os.makedirs(directory_path)
    return directory_path


def _save_model_plot(history, plots_path, append_title):
    # summarize history for accuracy
    title = "Accuracy_vs_Epoch_" + append_title
    plt.plot(history.history['acc'])
    plt.plot(history.history['val_acc'])
    plt.grid(True)
    plt.title(title)
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.savefig(os.path.join(plots_path, title + ".png"))
    plt.clf()

    # summarize history for loss
    title = "Loss_vs_Epoch_" + append_title
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.grid(True)
    plt.title(title)
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.savefig(os.path.join(plots_path, title + ".png"))
    plt.clf()


def weighted_binary_crossentropy(y_true, y_pred):
    """
    Custom Optimizer with weight around correct and incorrect logs

    :param y_true:
    :param y_pred:
    :return:
    """
    global true_weight, false_weight
    true_weight = 1 if true_weight is None else true_weight
    false_weight = 1 if false_weight is None else false_weight
    epsilon = K.epsilon()
    y_pred = K.clip(y_pred, epsilon, 1 - epsilon)
    out = -(true_weight * y_true * K.log(y_pred) + false_weight * (1.0 - y_true) * K.log(1.0 - y_pred))
    return K.mean(out, axis=-1)


def _conformal_eval(save_path):
    # training conformal prediction using validation data
    epsilon = [5, 2]
    mode = [0, 1]
    methods = [('Binary', Binary()), ('Ratio', Ratio()), ('Diff', Diff())]
    model = load_model(save_path,
                       custom_objects={'weighted_binary_crossentropy': weighted_binary_crossentropy})
    # methods = [('Ratio', Ratio())]
    for e in epsilon:
        for sigmoid_present in [True, False]:
            if sigmoid_present:
                _sub_text = 'with_sigmoid_output_'
                prediction_validation = model.predict(X_val, batch_size=batch_size)
                prediction_test = model.predict(X_test, batch_size=batch_size)
            else:
                _sub_text = 'with_logits_'
                pre_sigmoid_layer_model = Model(input=model.input,
                                                output=model.layers[-2].output)
                prediction_validation = pre_sigmoid_layer_model.predict(X_val, batch_size=batch_size)
                prediction_test = pre_sigmoid_layer_model.predict(X_test, batch_size=batch_size)
            for m in mode:
                if m == 0:
                    mode_name = 'Each_Class_threshold'
                else:
                    mode_name = 'Overall_threshold'
                _sub_text_mode = _sub_text + mode_name
                for name, method in methods:
                    _sub_text_method = _sub_text_mode + '_' + name
                    cf_save_path = _ensure_directory_exits(os.path.join(histogram_path,
                                                                        str(true_weight) + '_' + str(false_weight),
                                                                        str(e)))
                    cf_save_path = os.path.join(cf_save_path, _sub_text_method)
                    hist_data_save_path = os.path.join(histogram_data_path,
                                                       '_' + str(true_weight) + '_' + str(false_weight) +
                                                       str(e) + _sub_text_method + '.p')

                    cf = ConformalPrediction(prediction_validation.copy(), Y_val, epsilon=e, measure=method,
                                             threshold_mode=m)

                    # test data:
                    cf_prediction = cf.predict(prediction_test.copy())
                    cf_accuracy = cf.evaluate(cf_prediction, Y_test)
                    cf_label_histogram = cf.label_histogram(cf_prediction, cf_save_path, 'CP : ' + _sub_text_method)
                    print('Conformal Prediction with ' + name + '_' + mode_name + ':', cf_accuracy)
                    print('Labels Histogram:', cf_label_histogram)
                    cPickle.dump({
                        'histogram': cf_label_histogram,
                        'name': name,
                        'mode_name': mode_name,
                        'models_path': models_path,
                        'accuracy': cf_accuracy,
                        'sub_text': _sub_text_method
                    }, open(hist_data_save_path, 'wb'))


def _get_data():
    # the data, shuffled and split between train and test sets
    (X_train, y_train), (X_test, y_test) = cifar10.load_data()
    X_val, y_val = X_train[int(0.80 * len(X_train)):], y_train[int(0.80 * len(X_train)):]
    X_train, y_train = X_train[: int(0.80 * len(X_train))], y_train[:int(0.80 * len(X_train))]
    print('X_train shape:', X_train.shape)
    print(X_train.shape[0], 'train samples')
    print(X_test.shape[0], 'test samples')
    print(X_val.shape[0], 'Validation samples')

    # convert class vectors to binary class matrices
    Y_train = np_utils.to_categorical(y_train, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)
    Y_val = np_utils.to_categorical(y_val, nb_classes)

    X_train = X_train.astype('float32')
    X_val = X_val.astype('float32')
    X_test = X_test.astype('float32')
    X_train /= 255
    X_test /= 255
    X_val /= 255
    return (X_train, Y_train), (X_val, Y_val), (X_test, Y_test)


if __name__ == '__main__':
    # creating folders to save results
    data_path = _ensure_directory_exits(os.path.join(os.getcwd(), 'data_weighted'))
    models_path = _ensure_directory_exits(os.path.join(data_path, 'models'))
    histories_path = _ensure_directory_exits(os.path.join(data_path, 'history'))
    plots_path = _ensure_directory_exits(os.path.join(data_path, 'plots'))
    conformal_data_path = _ensure_directory_exits(os.path.join(os.getcwd(), 'conformal_data'))
    histogram_path = _ensure_directory_exits(os.path.join(conformal_data_path, 'histogram_1'))
    histogram_data_path = _ensure_directory_exits(os.path.join(conformal_data_path, 'data'))

    # initial_configuration
    batch_size, nb_classes, nb_epoch = 32, 10, 60
    # input image dimensions
    img_rows, img_cols, img_channels = 32, 32, 3

    (X_train, Y_train), (X_val, Y_val), (X_test, Y_test) = _get_data()
    X_val = X_val.transpose(0, 3, 1, 2)
    X_train = X_train.transpose(0, 3, 1, 2)
    X_test = X_test.transpose(0, 3, 1, 2)

    # true_weights = [0.9, 0.8, 0.7, 0.6]
    # false_weights = [0.1, 0.2, 0.3, 0.4]
    true_weights = [1]
    false_weights = [1]
    for f_w in false_weights:
        for t_w in true_weights:
            true_weight = t_w
            false_weight = f_w
            sub_title = 'weighted_loss_model'
            if true_weight != None and false_weight != None:
                sub_title += '-' + str(true_weight) + '_' + str(false_weight)
            model_save_path = os.path.join(models_path, 'cifar10_cnn_model_' + sub_title + '.h5')
            history_save_path = os.path.join(histories_path, 'cifar10_cnn_model_' + sub_title + '.p')
            # model = _create_model(img_channels, img_rows, img_cols, nb_classes)
            # optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)
            # model.compile(loss=weighted_binary_crossentropy, optimizer=optimizer, metrics=["accuracy"])
            # model, history = train(model, X_train, Y_train, X_test, Y_test, batch_size,
            #                        nb_epoch, model_save_path, history_save_path, data_augmentation=False)
            # _save_model_plot(history, plots_path, sub_title)
            print(model_save_path)
            if os.path.exists(model_save_path):
                print(model_save_path)
                _conformal_eval(model_save_path)

                # showing OverView of All the steps :
                # print('Overview of different Steps : ')
                # paths = [fn for fn in next(os.walk(histories_path))[2]]
                # for _file_path in paths:
                #     data = cPickle.load(open(os.path.join(histories_path, _file_path), 'rb'))
                #     print(_file_path.replace("_", " ") + ' :')
                #     print(data)
